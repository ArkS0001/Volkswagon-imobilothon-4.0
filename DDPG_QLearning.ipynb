{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjG5lzrvIKA/trFkAkFrIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Perfect-Cube/Volkswagon-imobilothon-4.0/blob/main/DDPG_QLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow gym numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm50YZxCl3L3",
        "outputId": "c3d49a26-9d05-499e-dcac-fbf45bf1fef2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (3.1.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "POYmZkXAlpV_",
        "outputId": "690aa03f-5766-4d87-c019-52c278e90433"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "invalid index to scalar variable.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c91ce669b4d2>\u001b[0m in \u001b[0;36m<cell line: 123>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatteryOptimizationEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-c91ce669b4d2>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c91ce669b4d2>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mbattery_usage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m  \u001b[0;31m# Simplified battery consumption model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbattery_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mbattery_usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ],
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Define the Environment\n",
        "class BatteryOptimizationEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(BatteryOptimizationEnv, self).__init__()\n",
        "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)  # Acceleration/deceleration\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)  # Battery, speed, etc.\n",
        "        self.state = np.array([1.0, 0.5, 0.0, 0.5, 0.5])  # Arbitrary initial state\n",
        "        self.battery_level = 1.0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([1.0, 0.5, 0.0, 0.5, 0.5])\n",
        "        self.battery_level = 1.0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        battery_usage = np.abs(action[0]) * 0.01  # Simplified battery consumption model\n",
        "        self.battery_level -= battery_usage\n",
        "        speed = self.state[1] + action[0] * 0.1\n",
        "        self.state = np.clip(np.array([self.battery_level, speed, self.state[2], self.state[3], self.state[4]]), 0, 1)\n",
        "        reward = -battery_usage\n",
        "        done = self.battery_level <= 0.1\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "# Define the DDPG Agent\n",
        "class DDPGAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.shape[0]\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "        self.buffer_capacity = 50000\n",
        "        self.batch_size = 64\n",
        "        self.noise_std_dev = 0.2\n",
        "        self.buffer = deque(maxlen=self.buffer_capacity)\n",
        "        self.actor_model = self.build_actor()\n",
        "        self.critic_model = self.build_critic()\n",
        "        self.target_actor = self.build_actor()\n",
        "        self.target_critic = self.build_critic()\n",
        "        self.update_target_networks(tau=1.0)\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "\n",
        "    def build_actor(self):\n",
        "        inputs = tf.keras.layers.Input(shape=(self.state_size,)) # The shape should be a tuple\n",
        "        out = tf.keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
        "        out = tf.keras.layers.Dense(256, activation=\"relu\")(out)\n",
        "        outputs = tf.keras.layers.Dense(self.action_size, activation=\"tanh\")(out)\n",
        "        return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    def build_critic(self):\n",
        "        state_input = tf.keras.layers.Input(shape=(self.state_size,)) # The shape should be a tuple\n",
        "        action_input = tf.keras.layers.Input(shape=(self.action_size,)) # The shape should be a tuple\n",
        "        concat = tf.keras.layers.Concatenate()([state_input, action_input])\n",
        "        out = tf.keras.layers.Dense(256, activation=\"relu\")(concat)\n",
        "        out = tf.keras.layers.Dense(256, activation=\"relu\")(out)\n",
        "        outputs = tf.keras.layers.Dense(1)(out)\n",
        "        return tf.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    def update_target_networks(self, tau=None):\n",
        "        tau = self.tau if tau is None else tau\n",
        "        for target_weights, main_weights in zip(self.target_actor.weights, self.actor_model.weights):\n",
        "            target_weights.assign(tau * main_weights + (1 - tau) * target_weights)\n",
        "        for target_weights, main_weights in zip(self.target_critic.weights, self.critic_model.weights):\n",
        "            target_weights.assign(tau * main_weights + (1 - tau) * target_weights)\n",
        "\n",
        "    def add_to_buffer(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample_from_buffer(self):\n",
        "        batch = random.sample(self.buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "        states, actions, rewards, next_states, dones = self.sample_from_buffer()\n",
        "        next_actions = self.target_actor(next_states)\n",
        "        target_qs = rewards + self.gamma * (1 - dones) * tf.squeeze(self.target_critic([next_states, next_actions]))\n",
        "        with tf.GradientTape() as tape:\n",
        "            critic_qs = tf.squeeze(self.critic_model([states, actions]))\n",
        "            critic_loss = tf.keras.losses.MSE(critic_qs, target_qs)\n",
        "        grads = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(grads, self.critic_model.trainable_variables))\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions_pred = self.actor_model(states)\n",
        "            actor_loss = -tf.reduce_mean(self.critic_model([states, actions_pred]))\n",
        "        grads = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(grads, self.actor_model.trainable_variables))\n",
        "        self.update_target_networks()\n",
        "\n",
        "    def act(self, state, noise=0.1):\n",
        "        state = np.expand_dims(state, axis=0)\n",
        "        action = tf.squeeze(self.actor_model(state)).numpy()\n",
        "        action += noise * np.random.normal(scale=self.noise_std_dev)\n",
        "        return np.clip(action, -1, 1)\n",
        "\n",
        "    def train_agent(self, episodes=1000):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                self.add_to_buffer((state, action, reward, next_state, done))\n",
        "                self.train()\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "            print(f\"Episode {episode+1}: Reward: {episode_reward}\")\n",
        "\n",
        "# Train the agent\n",
        "env = BatteryOptimizationEnv()\n",
        "agent = DDPGAgent(env)\n",
        "agent.train_agent(episodes=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "from collections import deque\n",
        "\n",
        "# Define the Environment\n",
        "class BatteryOptimizationEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(BatteryOptimizationEnv, self).__init__()\n",
        "        self.action_space = spaces.Box(low=-1.0, high=1.0, shape=(1,), dtype=np.float32)  # Acceleration/deceleration\n",
        "        self.observation_space = spaces.Box(low=0, high=1, shape=(5,), dtype=np.float32)  # Battery, speed, etc.\n",
        "        self.state = np.array([1.0, 0.5, 0.0, 0.5, 0.5])  # Arbitrary initial state\n",
        "        self.battery_level = 1.0\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.array([1.0, 0.5, 0.0, 0.5, 0.5])\n",
        "        self.battery_level = 1.0\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        battery_usage = np.abs(action[0]) * 0.01  # Simplified battery consumption model\n",
        "        self.battery_level -= battery_usage\n",
        "        speed = self.state[1] + action[0] * 0.1\n",
        "        self.state = np.clip(np.array([self.battery_level, speed, self.state[2], self.state[3], self.state[4]]), 0, 1)\n",
        "        reward = -battery_usage\n",
        "        done = self.battery_level <= 0.1\n",
        "        return self.state, reward, done, {}\n",
        "\n",
        "# Define the DDPG Agent\n",
        "class DDPGAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.shape[0]\n",
        "        self.gamma = 0.99\n",
        "        self.tau = 0.005\n",
        "        self.buffer_capacity = 50000\n",
        "        self.batch_size = 64\n",
        "        self.noise_std_dev = 0.2\n",
        "        self.buffer = deque(maxlen=self.buffer_capacity)\n",
        "        self.actor_model = self.build_actor()\n",
        "        self.critic_model = self.build_critic()\n",
        "        self.target_actor = self.build_actor()\n",
        "        self.target_critic = self.build_critic()\n",
        "        self.update_target_networks(tau=1.0)\n",
        "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "\n",
        "    def build_actor(self):\n",
        "        inputs = tf.keras.layers.Input(shape=(self.state_size,))\n",
        "        out = tf.keras.layers.Dense(256, activation=\"relu\")(inputs)\n",
        "        out = tf.keras.layers.Dense(256, activation=\"relu\")(out)\n",
        "        outputs = tf.keras.layers.Dense(self.action_size, activation=\"tanh\")(out)\n",
        "        return tf.keras.Model(inputs, outputs)\n",
        "\n",
        "    def build_critic(self):\n",
        "        state_input = tf.keras.layers.Input(shape=(self.state_size,))  # Corrected shape here\n",
        "        action_input = tf.keras.layers.Input(shape=(self.action_size,))\n",
        "        concat = tf.keras.layers.Concatenate()([state_input, action_input])\n",
        "        out = tf.keras.layers.Dense(256, activation=\"relu\")(concat)\n",
        "        out = tf.keras.layers.Dense(256, activation=\"relu\")(out)\n",
        "        outputs = tf.keras.layers.Dense(1)(out)\n",
        "        return tf.keras.Model([state_input, action_input], outputs)\n",
        "\n",
        "    def update_target_networks(self, tau=None):\n",
        "        tau = self.tau if tau is None else tau\n",
        "        for target_weights, main_weights in zip(self.target_actor.weights, self.actor_model.weights):\n",
        "            target_weights.assign(tau * main_weights + (1 - tau) * target_weights)\n",
        "        for target_weights, main_weights in zip(self.target_critic.weights, self.critic_model.weights):\n",
        "            target_weights.assign(tau * main_weights + (1 - tau) * target_weights)\n",
        "\n",
        "    def add_to_buffer(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample_from_buffer(self):\n",
        "        batch = random.sample(self.buffer, self.batch_size)\n",
        "        states, actions, rewards, next_states, dones = map(np.array, zip(*batch))\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.buffer) < self.batch_size:\n",
        "            return\n",
        "\n",
        "        # Sample a batch from the buffer\n",
        "        states, actions, rewards, next_states, dones = self.sample_from_buffer()\n",
        "\n",
        "        # Convert rewards and dones to tensors with the right shape\n",
        "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
        "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
        "\n",
        "        # Compute target Q-values\n",
        "        next_actions = self.target_actor(next_states)\n",
        "        target_qs = self.target_critic([next_states, next_actions])\n",
        "        target_qs = rewards + self.gamma * (1 - dones) * tf.squeeze(target_qs)\n",
        "\n",
        "        # Update Critic Network\n",
        "        with tf.GradientTape() as tape:\n",
        "            critic_qs = self.critic_model([states, actions])\n",
        "            critic_loss = tf.keras.losses.MSE(target_qs, tf.squeeze(critic_qs))\n",
        "        critic_grads = tape.gradient(critic_loss, self.critic_model.trainable_variables)\n",
        "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic_model.trainable_variables))\n",
        "\n",
        "        # Update Actor Network\n",
        "        with tf.GradientTape() as tape:\n",
        "            actions_pred = self.actor_model(states)\n",
        "            actor_loss = -tf.reduce_mean(self.critic_model([states, actions_pred]))\n",
        "        actor_grads = tape.gradient(actor_loss, self.actor_model.trainable_variables)\n",
        "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor_model.trainable_variables))\n",
        "\n",
        "        # Update target networks\n",
        "        self.update_target_networks()\n",
        "\n",
        "    def act(self, state, noise=0.1):\n",
        "      state = np.expand_dims(state, axis=0)\n",
        "      action = self.actor_model(state).numpy()  # Remove tf.squeeze\n",
        "      action += noise * np.random.normal(scale=self.noise_std_dev, size=self.action_size)  # Add size to noise\n",
        "      return np.clip(action, -1, 1)\n",
        "\n",
        "    def train_agent(self, episodes=1000):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            episode_reward = 0\n",
        "            done = False\n",
        "            while not done:\n",
        "                action = self.act(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "                self.add_to_buffer((state, action, reward, next_state, done))\n",
        "                self.train()\n",
        "                episode_reward += reward\n",
        "                state = next_state\n",
        "            print(f\"Episode {episode+1}: Reward: {episode_reward}\")\n",
        "\n",
        "# Train the agent\n",
        "env = BatteryOptimizationEnv()\n",
        "agent = DDPGAgent(env)\n",
        "agent.train_agent(episodes=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "GxqH9W5Gmblx",
        "outputId": "7edc1170-885a-43f4-9bbc-f4412544b7ae"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4b5954cd438f>\u001b[0m in \u001b[0;36m<cell line: 138>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatteryOptimizationEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDDPGAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-4b5954cd438f>\u001b[0m in \u001b[0;36mtrain_agent\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4b5954cd438f>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbattery_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mbattery_usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mspeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbattery_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspeed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mbattery_usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbattery_level\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (5,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define the Environment\n",
        "class BatteryOptimizationEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(BatteryOptimizationEnv, self).__init__()\n",
        "        # Discretize state and action spaces for Q-learning\n",
        "        self.battery_levels = np.linspace(0, 1, 11)  # Battery level from 0.0 to 1.0 in steps of 0.1\n",
        "        self.speeds = np.linspace(0, 1, 11)  # Speed level from 0.0 to 1.0 in steps of 0.1\n",
        "        self.action_space = spaces.Discrete(3)  # -1, 0, +1 acceleration values\n",
        "        self.observation_space = spaces.Discrete(len(self.battery_levels) * len(self.speeds))\n",
        "\n",
        "        # Define initial state\n",
        "        self.state = (10, 5)  # Initial battery level and speed index (1.0 battery, 0.5 speed)\n",
        "        self.done = False\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (10, 5)  # Reset battery to full and moderate speed\n",
        "        self.done = False\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        battery_idx, speed_idx = self.state\n",
        "\n",
        "        # Define battery consumption based on action\n",
        "        if action == 0:  # Decelerate\n",
        "            speed_idx = max(0, speed_idx - 1)\n",
        "            battery_consumed = 0.01\n",
        "        elif action == 1:  # Maintain speed\n",
        "            battery_consumed = 0.02\n",
        "        else:  # Accelerate\n",
        "            speed_idx = min(len(self.speeds) - 1, speed_idx + 1)\n",
        "            battery_consumed = 0.03\n",
        "\n",
        "        # Update battery level\n",
        "        battery_idx = max(0, battery_idx - int(battery_consumed * 10))\n",
        "        self.state = (battery_idx, speed_idx)\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = -battery_consumed\n",
        "        self.done = battery_idx <= 20  # End episode if battery level is too low\n",
        "\n",
        "        return self.state, reward, self.done, {}\n",
        "\n",
        "    def get_state_index(self, state):\n",
        "        battery_idx, speed_idx = state\n",
        "        return battery_idx * len(self.speeds) + speed_idx\n",
        "\n",
        "# Define the Q-learning Agent\n",
        "class QLearningAgent:\n",
        "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.995):\n",
        "        self.env = env\n",
        "        self.q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.exploration_rate = exploration_rate\n",
        "        self.exploration_decay = exploration_decay\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if random.uniform(0, 1) < self.exploration_rate:\n",
        "            return self.env.action_space.sample()  # Explore\n",
        "        state_index = self.env.get_state_index(state)\n",
        "        return np.argmax(self.q_table[state_index])  # Exploit\n",
        "\n",
        "    def update_q_table(self, state, action, reward, next_state):\n",
        "        state_index = self.env.get_state_index(state)\n",
        "        next_state_index = self.env.get_state_index(next_state)\n",
        "\n",
        "        # Q-learning update rule\n",
        "        best_next_action = np.argmax(self.q_table[next_state_index])\n",
        "        td_target = reward + self.discount_factor * self.q_table[next_state_index, best_next_action]\n",
        "        self.q_table[state_index, action] += self.learning_rate * (td_target - self.q_table[state_index, action])\n",
        "\n",
        "        # Print the Q-table after each update\n",
        "        print(f\"Updated Q-table for state {state} and action {action}:\\n{self.q_table}\")\n",
        "\n",
        "    def train(self, episodes=10):\n",
        "        for episode in range(episodes):\n",
        "            state = self.env.reset()\n",
        "            total_reward = 0\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _ = self.env.step(action)\n",
        "\n",
        "                # Update Q-table\n",
        "                self.update_q_table(state, action, reward, next_state)\n",
        "                total_reward += reward\n",
        "                state = next_state\n",
        "\n",
        "            # Decay exploration rate\n",
        "            self.exploration_rate *= self.exploration_decay\n",
        "            print(f\"Episode {episode + 1}: Total Reward: {total_reward}\")\n",
        "\n",
        "# Train the agent\n",
        "env = BatteryOptimizationEnv()\n",
        "agent = QLearningAgent(env)\n",
        "agent.train(episodes=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6kmCUE3of7l",
        "outputId": "9e2222b2-cb6a-4126-be23-82673e152f56"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Q-table for state (10, 5) and action 2:\n",
            "[[ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.    -0.003]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]]\n",
            "Episode 1: Total Reward: -0.03\n",
            "Updated Q-table for state (10, 5) and action 0:\n",
            "[[ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [-0.001  0.    -0.003]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]\n",
            " [ 0.     0.     0.   ]]\n",
            "Episode 2: Total Reward: -0.01\n",
            "Updated Q-table for state (10, 5) and action 2:\n",
            "[[ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [-0.001   0.     -0.0057]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]\n",
            " [ 0.      0.      0.    ]]\n",
            "Episode 3: Total Reward: -0.03\n",
            "Updated Q-table for state (10, 5) and action 2:\n",
            "[[ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [-0.001    0.      -0.00813]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]\n",
            " [ 0.       0.       0.     ]]\n",
            "Episode 4: Total Reward: -0.03\n",
            "Updated Q-table for state (10, 5) and action 2:\n",
            "[[ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [-0.001     0.       -0.010317]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]]\n",
            "Episode 5: Total Reward: -0.03\n",
            "Updated Q-table for state (10, 5) and action 1:\n",
            "[[ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [-0.001    -0.002    -0.010317]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]]\n",
            "Episode 6: Total Reward: -0.02\n",
            "Updated Q-table for state (10, 5) and action 0:\n",
            "[[ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [-0.0019   -0.002    -0.010317]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]]\n",
            "Episode 7: Total Reward: -0.01\n",
            "Updated Q-table for state (10, 5) and action 0:\n",
            "[[ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [-0.00271  -0.002    -0.010317]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]]\n",
            "Episode 8: Total Reward: -0.01\n",
            "Updated Q-table for state (10, 5) and action 0:\n",
            "[[ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [-0.003439 -0.002    -0.010317]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]\n",
            " [ 0.        0.        0.      ]]\n",
            "Episode 9: Total Reward: -0.01\n",
            "Updated Q-table for state (10, 5) and action 2:\n",
            "[[ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [-0.003439  -0.002     -0.0122853]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]\n",
            " [ 0.         0.         0.       ]]\n",
            "Episode 10: Total Reward: -0.03\n"
          ]
        }
      ]
    }
  ]
}